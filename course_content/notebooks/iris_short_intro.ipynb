{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A (short) introduction to Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import iris\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n",
      "1.11.2\n"
     ]
    }
   ],
   "source": [
    "print(iris.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss ###\n",
    "\n",
    "What is Iris?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris and the cube\n",
    "\n",
    "The top level object in Iris is called a cube. A cube contains data and metadata about a single phenomenon and is an implementation of the data model interpreted from the *Climate and Forecast (CF) Metadata Conventions*.\n",
    "\n",
    "Each cube has:\n",
    "\n",
    " * A data array (typically a NumPy array).\n",
    " * A \"name\", preferably a CF \"standard name\" to describe the phenomenon that the cube represents.\n",
    " * A collection of coordinates to describe each of the dimensions of the data array. These coordinates are split into two types:\n",
    "    * Dimensioned coordinates are numeric, monotonic and represent a single dimension of the data array. There may be only one dimensioned coordinate per data dimension.\n",
    "    * Auxilliary coordinates can be of any type, including discrete values such as strings, and may represent more than one data dimension.\n",
    "\n",
    "A fuller explanation is available in the [Iris user guide](http://scitools.org.uk/iris/docs/latest/userguide/iris_cubes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a simple example to demonstrate the cube concept.\n",
    "\n",
    "Suppose we have a ``(3, 2, 4)`` NumPy array:\n",
    "\n",
    "![](../images/multi_array.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where dimensions 0, 1, and 2 have lengths 3, 2 and 4 respectively.\n",
    "\n",
    "The Iris cube to represent this data may consist of:\n",
    "\n",
    " * a standard name of \"air_temperature\" and units of \"kelvin\"\n",
    "\n",
    " * a data array of shape ``(3, 2, 4)``\n",
    "\n",
    " * a coordinate, mapping to dimension 0, consisting of:\n",
    "     * a standard name of \"height\" and units of \"meters\"\n",
    "     * an array of length 3 representing the 3 height points\n",
    "     \n",
    " * a coordinate, mapping to dimension 1, consisting of:\n",
    "     * a standard name of \"latitude\" and units of \"degrees\"\n",
    "     * an array of length 2 representing the 2 latitude points\n",
    "     * a coordinate system such that the latitude points could be fully located on the globe\n",
    "     \n",
    " * a coordinate, mapping to dimension 2, consisting of:\n",
    "     * a standard name of \"longitude\" and units of \"degrees\"\n",
    "     * an array of length 4 representing the 4 longitude points\n",
    "     * a coordinate system such that the longitude points could be fully located on the globe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pictorially the cube has taken on more information than a simple array:\n",
    "\n",
    "![](../images/multi_array_to_cube.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst it is possible to construct a cube by hand, a far more common approach to getting hold of a cube is to use the Iris load function to access data that already exists in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n",
      "1: surface_altitude / (m)              (grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cubes = iris.load(fname)\n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we've loaded two cubes, one representing the \"surface_altitude\" and the other representing \"air_potential_temperature\". We can infer even more detail from this printout; for example, what are the dimensions and shape of the \"air_potential_temperature\" cube?\n",
    "\n",
    "Above we've printed the ``iris.cube.CubeList`` instance representing all of the cubes found in the given filename. However, we can see more detail by printing individual cubes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n",
      "     Dimension coordinates:\n",
      "          time                           x                      -                 -                    -\n",
      "          model_level_number             -                      x                 -                    -\n",
      "          grid_latitude                  -                      -                 x                    -\n",
      "          grid_longitude                 -                      -                 -                    x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_period                x                      -                 -                    -\n",
      "          level_height                   -                      x                 -                    -\n",
      "          sigma                          -                      x                 -                    -\n",
      "          surface_altitude               -                      -                 x                    x\n",
      "     Derived coordinates:\n",
      "          altitude                       -                      x                 x                    x\n",
      "     Scalar coordinates:\n",
      "          forecast_reference_time: 2009-11-19 04:00:00\n",
      "     Attributes:\n",
      "          STASH: m01s00i004\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 7.3\n"
     ]
    }
   ],
   "source": [
    "air_pot_temp = cubes[0]\n",
    "print(air_pot_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dig even deeper and print individual coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DimCoord(array([ 1,  4,  7, 10, 13, 16, 19], dtype=int32), standard_name='model_level_number', units=Unit('1'), attributes={'positive': 'up'})\n"
     ]
    }
   ],
   "source": [
    "print(air_pot_temp.coord('model_level_number'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cube attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_temperature / (K)               (time: 240; latitude: 37; longitude: 49)\n",
      "     Dimension coordinates:\n",
      "          time                           x              -              -\n",
      "          latitude                       -              x              -\n",
      "          longitude                      -              -              x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_period                x              -              -\n",
      "     Scalar coordinates:\n",
      "          forecast_reference_time: 1859-09-01 06:00:00\n",
      "          height: 1.5 m\n",
      "     Attributes:\n",
      "          Conventions: CF-1.5\n",
      "          Model scenario: A1B\n",
      "          STASH: m01s03i236\n",
      "          source: Data from Met Office Unified Model 6.05\n",
      "     Cell methods:\n",
      "          mean: time (6 hour)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/fileformats/cf.py:1140: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n"
     ]
    }
   ],
   "source": [
    "cube = iris.load_cube(iris.sample_data_path('A1B_north_america.nc'))\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a cube's data array the ``data`` property exists. This is either a NumPy array or in some cases a NumPy masked array. It is very important to note that for most of the supported filetypes in Iris, the cube's data isn't actually loaded until you request it via this property (either directly or indirectly). After you've accessed the data once, it is stored on the cube and thus won't be loaded from disk again.\n",
    "\n",
    "To find the shape of a cube's data it is possible to call ``cube.data.shape`` or ``cube.data.ndim``, but this will trigger any unloaded data to be loaded. Therefore ``shape`` and ``ndim`` are properties available directly on the cube that do not unnecessarily load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 37, 49)\n",
      "3\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(cube.shape)\n",
    "print(cube.ndim)\n",
    "print(type(cube.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``standard_name``, ``long_name`` and to an extent ``var_name`` are all attributes to describe the phenomenon that the cube represents. The ``name()`` method is a convenience that looks at the name attributes in the order they are listed above, returning the first non-empty string. To rename a cube, it is possible to set the attributes manually, but it is generally easier to use the ``rename()`` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_temperature\n",
      "None\n",
      "air_temperature\n",
      "air_temperature\n"
     ]
    }
   ],
   "source": [
    "print(cube.standard_name)\n",
    "print(cube.long_name)\n",
    "print(cube.var_name)\n",
    "print(cube.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cube.rename(\"A name that isn't a valid CF standard name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "A name that isn't a valid CF standard name\n",
      "None\n",
      "A name that isn't a valid CF standard name\n"
     ]
    }
   ],
   "source": [
    "print(cube.standard_name)\n",
    "print(cube.long_name)\n",
    "print(cube.var_name)\n",
    "print(cube.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``units`` attribute on a cube tells us the units of the numbers held in the data array. We can manually change the units, or better, we can convert the cube to another unit using the ``convert_units`` method, which will automatically update the data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K\n",
      "306.073\n",
      "Celsius\n",
      "32.9233\n"
     ]
    }
   ],
   "source": [
    "print(cube.units)\n",
    "print(cube.data.max())\n",
    "cube.convert_units('Celsius')\n",
    "print(cube.units)\n",
    "print(cube.data.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cube has a dictionary for extra general purpose attributes, which can be accessed with the ``cube.attributes`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'Data from Met Office Unified Model 6.05', 'STASH': STASH(model=1, section=3, item=236), 'Model scenario': 'A1B', 'Conventions': 'CF-1.5'}\n",
      "m01s03i236\n"
     ]
    }
   ],
   "source": [
    "print(cube.attributes)\n",
    "print(cube.attributes['STASH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A less frequently used attribute on a cube is its ``cell_methods``. The cell methods are a way to store information about the processing that has taken place on the cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: time (6 hour)\n"
     ]
    }
   ],
   "source": [
    "for cell_method in cube.cell_methods:\n",
    "    print(cell_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the cube is has been produced by taking a mean of forecasts sampled at 6 hourly intervals (we need to look at the time coordinate to identify any more information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates\n",
    "\n",
    "As we've seen, cubes need coordinate information to help us describe the underlying phenomenon. Typically a cube's coordinates are accessed with the ``coords`` or ``coord`` methods. The latter *must* return exactly one coordinate for the given parameter filters, where the former returns a list of matching coordinates, possibly of length 0.\n",
    "\n",
    "For example, to access the time coordinate, and print the first 4 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DimCoord([1860-06-01 00:00:00, 1861-06-01 00:00:00, 1862-06-01 00:00:00,\n",
      "       1863-06-01 00:00:00], bounds=[[1859-12-01 00:00:00, 1860-12-01 00:00:00],\n",
      "       [1860-12-01 00:00:00, 1861-12-01 00:00:00],\n",
      "       [1861-12-01 00:00:00, 1862-12-01 00:00:00],\n",
      "       [1862-12-01 00:00:00, 1863-12-01 00:00:00]], standard_name='time', calendar='360_day', var_name='time')\n"
     ]
    }
   ],
   "source": [
    "time = cube.coord('time')\n",
    "print(time[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the cell method from the previous section, we can now see that this cube represents the mean annual air temperature, sampled every 6 hours, starting in 1860."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinate interface is very similar to that of a cube. The attributes that exist on both cubes and coordinates are: ``standard_name``, ``long_name``, ``var_name``, ``units``, ``attributes`` and ``shape``. Similarly, the ``name()``, ``rename()`` and ``convert_units()`` methods also exist on a coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A coordinate does not have ``data``, instead it has ``points`` and ``bounds`` (``bounds`` may be ``None``). In Iris, time coordinates are currently represented as \"a number since an epoch\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit('hours since 1970-01-01 00:00:00', calendar='360_day')\n",
      "[-946800. -938160. -929520. -920880.]\n",
      "[[-951120. -942480.]\n",
      " [-942480. -933840.]\n",
      " [-933840. -925200.]\n",
      " [-925200. -916560.]]\n"
     ]
    }
   ],
   "source": [
    "print(repr(time.units))\n",
    "print(time.points[:4])\n",
    "print(time.bounds[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers can be converted to datetime objects with the unit's ``num2date`` method. Dates can be converted back again with the ``date2num`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1860-06-01 00:00:00 1861-06-01 00:00:00 1862-06-01 00:00:00\n",
      " 1863-06-01 00:00:00]\n",
      "720.0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(time.units.num2date(time.points[:4]))\n",
    "print(time.units.date2num(datetime.datetime(1970, 2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important attribute on a coordinate is its coordinate system. Coordinate systems may be ``None`` for trivial coordinates, but particularly for spatial coordinates, they may be complex definitions of things such as the projection, ellipse and/or datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeogCS(6371229.0)\n"
     ]
    }
   ],
   "source": [
    "lat = cube.coord('latitude')\n",
    "print(lat.coord_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the latitude's coordinate system is a simple geographic latitude on a spherical globe of radius 6371229 (meters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is desirable to add bounds to a coordinate that doesn't have any. This is often the case for creating \"block\" type plots where a coordinate should be able to represent an interval of values, rather than a single point. The ``guess_bounds`` method on a coordinate is useful in this regard. For example, the latitude coordinate previously obtained does not have bounds, but we can either set some manually, or use the ``guess_bounds`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 15.    16.25  17.5   18.75]\n",
      "None\n",
      "[[ 14.375  15.625]\n",
      " [ 15.625  16.875]\n",
      " [ 16.875  18.125]\n",
      " [ 18.125  19.375]]\n"
     ]
    }
   ],
   "source": [
    "print(lat.points[:4])\n",
    "print(lat.bounds)\n",
    "if lat.bounds is None:\n",
    "    lat.guess_bounds()\n",
    "print(lat.bounds[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "1\\. Using the file in ``iris.sample_data_path('atlantic_profiles.nc')`` load the data and print the cube list. Store these cubes in a variable called cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Loop through each of the cubes (e.g. ``for cube in cubes``) and print the standard name of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Extract the \"sea_water_potential_temperature\" cube. Print the minimum, maximum, mean and standard deviation of the cube's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Print the attributes of the cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Print the names of all coordinates on the cube. (Hint: Remember the cube.coords method without any keywords will give us all of the cube's coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Get hold of the \"latitude\" coordinate on the cube. Identify whether this coordinate has bounds. Print the minimum and maximum latitude points in the cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen the basic ``load`` function, but we can also control which cubes are actually loaded with *constraints*. The simplest constraint is just a string, which filters cubes based on their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "print(iris.load(fname, 'air_potential_temperature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on sample_data_path:\n",
    "\n",
    "Throughout this course we will make use of the sample data that Iris provides. The function ``iris.sample_data_path`` returns the appropriate path to the file in the Iris sample data collection. A common mistake for Iris users is to use the ``sample_data_path`` function to access data that is not part of Iris's sample data collection - this is bad practice and is unlikely to work in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Print the result of ``iris.sample_data_path('uk_hires.pp')`` to verify that it returns a string pointing to a file on your system. Use this string directly in the call to ``iris.load`` and confirm the result is the same as in the previous example e.g.:\n",
    "\n",
    "    print iris.load('/path/to/iris/sampledata/uk_hires.pp', 'air_potential_temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The three load functions: load, load_cube and load_cubes\n",
    "\n",
    "There are three main load functions in Iris: ``load``, ``load_cube`` and ``load_cubes``.\n",
    "\n",
    "1. **load** is a general purpose loading function. Typically this is where all data analysis will start, before more loading is refined with the more controlled loading from the other two functions.\n",
    "2. **load_cube** returns a single cube from the given source(s) and constraint. There will be exactly one cube, or an exception will be raised.\n",
    "3. **load_cubes** returns a list of cubes from the given sources(s) and constraint(s). There will be exactly one cube per constraint, or an exception will be raised.\n",
    "\n",
    "\n",
    "Note: ``load_cube`` is a special case of ``load_cubes``, which can be seen with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, = iris.load(fname, 'surface_altitude')\n",
    "c2 = iris.load_cube(fname, 'surface_altitude')\n",
    "c3, = iris.load_cubes(fname, 'surface_altitude')\n",
    "c1 == c2 == c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it is a good idea to make use of the ``load_cube``/``load_cubes`` functions rather than the generic ``load`` function in non-exploratory code. Doing so makes your code more resilient to changes in the data source, often results in more readable/maintainable code, and in combination with well defined constraints, often leads to improve load performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load functions all accept a list of filenames to load, and any of the filenames can be \"glob\" patterns (http://docs.python.org/2/library/glob.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 (continued)\n",
    "\n",
    "Read in the files found at **``iris.sample_data_path('GloSea4', 'ensemble_010.pp')``** and\n",
    "**``iris.sample_data_path('GloSea4', 'ensemble_011.pp')``** using a single load call. Do this by:\n",
    "\n",
    "1\\. providing a list of the two filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. providing a suitable glob pattern. (Notice that **``iris.load(iris.sample_data_path('GloSea4', 'ensemble_01*.pp'))``** picks up too many files.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints\n",
    "\n",
    "Iris's constraints mechanism provides a powerful way to filter a subset of data from a larger collection. We've already seen that constraints can be used at load time to return data of interest from a file, but we can also apply constraints to a single cube, or a list of cubes, using their respective ``extract`` methods:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cubes = iris.load(fname)\n",
    "print(cubes.extract('air_potential_temperature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest constraint, namely a string that matches a cube's name, is conveniently converted into an actual ``iris.Constraint`` instance wherever needed. However, we could construct this constraint manually and compare with the previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "pot_temperature_constraint = iris.Constraint('air_potential_temperature')\n",
    "print(cubes.extract(pot_temperature_constraint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Constraint constructor also takes arbitrary keywords to constrain coordinate values. For example, to extract model level number 10 from the air potential temperature cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "pot_temperature_constraint = iris.Constraint('air_potential_temperature',\n",
    "                                             model_level_number=10)\n",
    "print(cubes.extract(pot_temperature_constraint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a list of possible values, and even combine two constraints with ``&``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 2; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "print(cubes.extract('air_potential_temperature' & \n",
    "                    iris.Constraint(model_level_number=[4, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define arbitrary functions that operate on each cell of a coordinate. This is a common thing to do for floating point coordinates, where exact equality is non-trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_potential_temperature / (K)     (time: 3; model_level_number: 3; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "def less_than_10(cell):\n",
    "    \"\"\"Return True for values that are less than 10.\"\"\"\n",
    "    return cell < 10\n",
    "\n",
    "print(cubes.extract(iris.Constraint('air_potential_temperature',\n",
    "                                    model_level_number=less_than_10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Iris cells represent both point and bound, cell comparison can sometimes be counter-intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell > 12 is False\n",
      "cell >= 12 is True\n",
      "cell == 12 is True\n",
      "cell <= 12 is True\n",
      "cell < 12 is False\n"
     ]
    }
   ],
   "source": [
    "def cell_comparison(cell, value):\n",
    "    print('cell > {0!r} is {1}'.format(value, cell > value))\n",
    "    print('cell >= {0!r} is {1}'.format(value, cell >= value))\n",
    "    print('cell == {0!r} is {1}'.format(value, cell == value))\n",
    "    print('cell <= {0!r} is {1}'.format(value, cell <= value))\n",
    "    print('cell < {0!r} is {1}'.format(value, cell < value))\n",
    "\n",
    "cell = iris.coords.Cell(point=10, bound=[8, 12])\n",
    "cell_comparison(cell, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want full control of how cell comparison is taking place, you can always compare with another cell, or just access the cell's individual ``point`` or ``bound``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell > 11 is False\n",
      "cell >= 11 is True\n",
      "cell == 11 is True\n",
      "cell <= 11 is True\n",
      "cell < 11 is False\n",
      "\n",
      "cell > Cell(point=11, bound=None) is False\n",
      "cell >= Cell(point=11, bound=None) is False\n",
      "cell == Cell(point=11, bound=None) is False\n",
      "cell <= Cell(point=11, bound=None) is True\n",
      "cell < Cell(point=11, bound=None) is True\n"
     ]
    }
   ],
   "source": [
    "cell_1 = iris.coords.Cell(point=10, bound=[8, 12])\n",
    "cell_2 = iris.coords.Cell(point=11, bound=None)\n",
    "\n",
    "cell_comparison(cell_1, 11)\n",
    "print()\n",
    "cell_comparison(cell_1, cell_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to want to build a constraint for time. With Iris < v1.6 it was harder to build time constraints than we would have liked, because of the way that time coordinates had been implemented.\n",
    "\n",
    "However, since v1.6 this has been made simpler through the ability to compare against cells containing datetimes. The functionality can be enabled globally within the session (and will be enabled by default in future release of Iris) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.FUTURE.cell_datetime_objects = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this set, it is now possible to do the same constraint by simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (time: 2; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "time_constraint = iris.Constraint(\n",
    "    time=lambda c: c >= datetime.datetime(2009, 11, 19, 11, 0))\n",
    "print(air_pot_temp.extract(time_constraint).summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently still some limitations however. For example, it is not yet possible to do cell based datetime comparisons when the datetime is from anything other than a Gregorian calendar (e.g. such as the 360-day calendar often used in climate models). When this is the case however, we can always access individual components of the datetime and do comparisons on those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "time_constraint = iris.Constraint(time=lambda c: c.point.hour == 11)\n",
    "print(air_pot_temp.extract(time_constraint).summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further functionality has been added to isolate individual components of a datetime via the [PartialDateTime](http://scitools.org.uk/iris/docs/latest/iris/iris/time.html?highlight=partial#iris.time.PartialDateTime) class. In this case we can extract the timestep at the 11th hour with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n",
      "DimCoord([2009-11-19 11:00:00], standard_name='time', calendar='gregorian')\n"
     ]
    }
   ],
   "source": [
    "from iris.time import PartialDateTime\n",
    "eleventh_hour = iris.Constraint(time=PartialDateTime(hour=11))\n",
    "print(air_pot_temp.extract(eleventh_hour).summary(True))\n",
    "print(air_pot_temp.extract(eleventh_hour).coord('time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "The following function tells us whether or not a cube has cell methods:\n",
    "\n",
    "    def has_cell_methods(cube):\n",
    "        return len(cube.cell_methods) > 0\n",
    "\n",
    "1\\. With the cubes loaded from ``[iris.sample_data_path('A1B_north_america.nc'), iris.sample_data_path('uk_hires.pp')]`` use the CubeList's **``extract``** method to filter only the cubes that have cell methods. (Hint: Look at the ``iris.Constraint`` documentation for the **cube_func** keyword). You should find that the 3 cubes are whittled down to just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the file found at ``iris.sample_data_path('A1B_north_america.nc')`` filter the cube, using constraints, such that only data between 1860 and 1980 remains (hint: This data has a 360-day calendar with yearly data from 1860 to 2100, so we will need to access the individual components of the cell point's datetime, to return a time dimension of length 120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving cubes\n",
    "\n",
    "The ``iris.save`` function provides a convenient interface to save Cube and CubeList instances.\n",
    "\n",
    "To save some cubes to a NetCDF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/fileformats/netcdf.py:2094: IrisDeprecation: NetCDF default saving behaviour currently assigns the outermost dimensions to unlimited. This behaviour is to be deprecated, in favour of no automatic assignment. To switch to the new behaviour, set iris.FUTURE.netcdf_no_unlimited to True.\n",
      "  warn_deprecated(msg)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cubes = iris.load(fname)\n",
    "iris.save(cubes, 'saved_cubes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: ncdump: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!ncdump -h saved_cubes.nc | head -n 20\n",
    "!rm saved_cubes.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra keywords can be passed to specific fileformat savers.\n",
    "\n",
    "**Task:** Go to the Iris reference documentation for ``iris.save``. What fileformats can Iris currently save to? What keywords are accepted to ``iris.save`` when saving a PP file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Cubes can be indexed in a familiar manner to that of NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (time: 3; model_level_number: 7; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname, 'air_potential_temperature')\n",
    "print(cube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air_potential_temperature / (K)     (time: 3; model_level_number: 4; grid_latitude: 20; grid_longitude: 10)'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subcube = cube[..., ::2, 15:35, :10]\n",
    "subcube.summary(shorten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the result of indexing a cube is *always* a copy and never a *view* on the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate ###\n",
    "\n",
    "We have seen that merge combines a list of cubes with a common scalar coordinate to produce a single cube with a new dimension created from these scalar values.\n",
    "\n",
    "But what happens if you try to combine cubes along a common dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_temperature / (K)               (time: 10; latitude: 37; longitude: 49)\n",
      "1: air_temperature / (K)               (time: 10; latitude: 37; longitude: 49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/fileformats/cf.py:1140: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('A1B_north_america.nc')\n",
    "cube = iris.load_cube(fname)\n",
    "\n",
    "cube_1 = cube[:10]\n",
    "cube_2 = cube[10:20]\n",
    "cubes = iris.cube.CubeList([cube_1, cube_2])\n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cubes should be able to be merged; after all, they have both come from the same original cube!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_temperature / (K)               (time: 10; latitude: 37; longitude: 49)\n",
      "1: air_temperature / (K)               (time: 10; latitude: 37; longitude: 49)\n"
     ]
    }
   ],
   "source": [
    "print(cubes.merge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge cannot be used to combine common non-scalar coordinates. Instead we must use concatenate, which joins together (\"concatenates\") common non-scalar coordinates to produce a single cube with the common dimension extended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: air_temperature / (K)               (time: 20; latitude: 37; longitude: 49)\n"
     ]
    }
   ],
   "source": [
    "print(cubes.concatenate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with merge, Iris contains functionality to simplify the identification process for causes of failed concatenations. The ``concatenate_cube`` method of a CubeList expects the list of cubes to contain only cubes that can be concatenated to produce a single cube. If they do not concatenate to a single cube, a descriptive error will be raised. For instance:\n",
    "\n",
    "```\n",
    "    >>> print cubes.concatenate_cube()\n",
    "    Traceback (most recent call last):\n",
    "      ...\n",
    "    iris.exceptions.ConcatenateError: failed to concatenate into a single cube.\n",
    "      Scalar coordinates differ: forecast_reference_time, height != forecast_reference_time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration\n",
    "\n",
    "We can loop through all desired subcubes in a larger cube using the ``slices`` and ``slices_over`` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname,\n",
    "                      iris.Constraint('air_potential_temperature',\n",
    "                                      model_level_number=1))\n",
    "print(cube.summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``slices`` method returns all the slices of a cube on the dimensions specified by the coordinates passed to the slices method.\n",
    "\n",
    "So in this example, each grid_latitude / grid_longitude slice of the cube is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "for subcube in cube.slices(['grid_latitude', 'grid_longitude']):\n",
    "    print(subcube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``iris.iterate.izip`` function extends this concept and allows us to loop through multiple cubes at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_temperature / (K)               (latitude: 37; longitude: 49) 1860-06-01 00:00:00\n",
      "air_temperature / (K)               (latitude: 37; longitude: 49) 1860-06-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/fileformats/cf.py:1140: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n",
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/fileformats/cf.py:1140: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n"
     ]
    }
   ],
   "source": [
    "from iris.iterate import izip\n",
    "\n",
    "iris.FUTURE.cell_datetime_objects = True\n",
    "\n",
    "e1 = iris.load_cube(iris.sample_data_path('E1_north_america.nc'))\n",
    "a1b = iris.load_cube(iris.sample_data_path('A1B_north_america.nc'))\n",
    "\n",
    "for e1_slice, a1b_slice in izip(e1, a1b, coords=['latitude', 'longitude']):\n",
    "    print(e1_slice.summary(True), e1_slice.coord('time').cell(0).point)\n",
    "    print(a1b_slice.summary(True), a1b_slice.coord('time').cell(0).point)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, one real use for this functionality would be to plot the ``e1`` cube next to the ``a1b`` cube for each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ``slices_over`` to return one subcube for each coordinate value in a specified coordinate. This helps us when trying to retrieve all the slices along a given cube dimension.\n",
    "\n",
    "For example, let's consider retrieving all the slices over the time dimension (i.e. each time step in its own cube with a scalar time coordinate) using ``slices``. As per the above example, to achieve this using ``slices`` we would have to specify all the cube's dimensions _except_ the time dimension.\n",
    "\n",
    "Let's take a look at ``slices_over`` providing this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n",
      "air_potential_temperature / (K)     (time: 3; grid_latitude: 204; grid_longitude: 187)\n"
     ]
    }
   ],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname, 'air_potential_temperature')\n",
    "for subcube in cube.slices_over('model_level_number'):\n",
    "    print(subcube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cube maths\n",
    "\n",
    "Basic mathematical operators exist on the cube to allow one to add, subtract, divide, multiply and perform other mathematical operations on cubes of a similar shape to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_temperature / (K)               (time: 240; latitude: 37; longitude: 49)\n",
      "air_temperature / (K)               (time: 240; latitude: 37; longitude: 49)\n",
      "     Dimension coordinates:\n",
      "          time                           x              -              -\n",
      "          latitude                       -              x              -\n",
      "          longitude                      -              -              x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_period                x              -              -\n",
      "     Scalar coordinates:\n",
      "          forecast_reference_time: 1859-09-01 06:00:00\n",
      "          height: 1.5 m\n",
      "     Attributes:\n",
      "          Conventions: CF-1.5\n",
      "          Model scenario: A1B\n",
      "          STASH: m01s03i236\n",
      "          source: Data from Met Office Unified Model 6.05\n",
      "     Cell methods:\n",
      "          mean: time (6 hour)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/fileformats/cf.py:1140: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n",
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/fileformats/cf.py:1140: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n"
     ]
    }
   ],
   "source": [
    "a1b = iris.load_cube(iris.sample_data_path('A1B_north_america.nc'))\n",
    "e1 = iris.load_cube(iris.sample_data_path('E1_north_america.nc'))\n",
    "\n",
    "print(e1.summary(True))\n",
    "print(a1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown / (K)                       (time: 240; latitude: 37; longitude: 49)\n",
      "     Dimension coordinates:\n",
      "          time                           x              -              -\n",
      "          latitude                       -              x              -\n",
      "          longitude                      -              -              x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_period                x              -              -\n",
      "     Scalar coordinates:\n",
      "          forecast_reference_time: 1859-09-01 06:00:00\n",
      "          height: 1.5 m\n"
     ]
    }
   ],
   "source": [
    "scenario_difference = a1b - e1\n",
    "print(scenario_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resultant cube's name is now unknown and that the coordinates time and forecast_period have been removed; this is because these coordinates differed between the two input cubes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to operate on cubes with numeric scalars, NumPy arrays and even cube coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<iris 'Cube' of unknown / (0.0174532925199433 kelvin-radian) (time: 240; latitude: 37; longitude: 49)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 * e1.coord('latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cube broadcasting is also taking place, meaning that the two cubes don't need to have the same shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown / (K)                       (time: 240; latitude: 37; longitude: 49)\n",
      "     Dimension coordinates:\n",
      "          time                           x              -              -\n",
      "          latitude                       -              x              -\n",
      "          longitude                      -              -              x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_period                x              -              -\n",
      "     Scalar coordinates:\n",
      "          forecast_reference_time: 1859-09-01 06:00:00\n",
      "          height: 1.5 m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/coords.py:992: UserWarning: Collapsing a non-contiguous coordinate. Metadata may not be fully descriptive for 'forecast_period'.\n",
      "  warnings.warn(msg.format(self.name()))\n"
     ]
    }
   ],
   "source": [
    "print(e1 - e1.collapsed('time', iris.analysis.MEAN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes aggregations don't exist in Iris, so it is important that we still have the power to update the cube's data directly. Whenever we do this though, we should be mindful of updating the necessary metadata on the cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e1_hot = e1.copy()\n",
    "\n",
    "e1_hot.data = np.ma.masked_less_equal(e1_hot.data, 280)\n",
    "e1_hot.rename('air temperatures greater than 280K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating extra annotation coordinates for statistical convenience\n",
    "\n",
    "Sometimes we want to be able to categorise data before performing statistical operations on it. For example, we might want to categorise our data by \"daylight maximum\" or \"seasonal mean\" etc. Both of these categorisations would be based on the time coordinate.\n",
    "\n",
    "The ``iris.coord_categorisation`` module provides convenience functions to add some common categorical coordinates, and provides a generalised function to allow each creation of custom categorisations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/fileformats/cf.py:1140: IrisDeprecation: NetCDF default loading behaviour currently does not expose variables which define reference surfaces for dimensionless vertical coordinates as independent Cubes. This behaviour is deprecated in favour of automatic promotion to Cubes. To switch to the new behaviour, set iris.FUTURE.netcdf_promote to True.\n",
      "  warn_deprecated(msg)\n"
     ]
    }
   ],
   "source": [
    "import iris.coord_categorisation as coord_cat\n",
    "\n",
    "filename = iris.sample_data_path('ostia_monthly.nc')\n",
    "cube = iris.load_cube(filename, 'surface_temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cube loaded represents the monthly air_temperature from April 2006 through to October 2010. Let's add a categorisation coordinate to this cube to identify the climatological season (i.e \"djf\", \"mam\", \"jja\" or \"son\") of each time point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coord_cat.add_season(cube, 'time', name='clim_season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the cube's ``aggregated_by`` method to \"group by and aggregate\" on the season, to produce the seasonal mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/coords.py:495: VisibleDeprecationWarning: an index can only have a single Ellipsis (`...`); replace all but one with slices (`:`).\n",
      "  bounds = bounds[keys + (Ellipsis, )]\n"
     ]
    }
   ],
   "source": [
    "seasonal_mean = cube.aggregated_by('clim_season', iris.analysis.MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this further by extracting by our newly created coordinate, producing a plot of the winter zonal mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/cube.py:3254: UserWarning: Collapsing spatial coordinate 'latitude' without weighting\n",
      "  warnings.warn(msg.format(coord.name()))\n",
      "/Users/watson-parris/anaconda/envs/cis_test/lib/python3.4/site-packages/iris/coords.py:992: UserWarning: Collapsing a non-contiguous coordinate. Metadata may not be fully descriptive for 'latitude'.\n",
      "  warnings.warn(msg.format(self.name()))\n"
     ]
    }
   ],
   "source": [
    "winter = seasonal_mean.extract(iris.Constraint(clim_season='djf'))\n",
    "\n",
    "qplt.plot(winter.collapsed('latitude', iris.analysis.MEAN))\n",
    "plt.title('Winter zonal mean surface temperature at $\\pm5^{\\circ}$ latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom categorisation ###\n",
    "\n",
    "Custom categorisation can be achieved with an arbitrary function. For example, the existing ``add_year`` categorisor takes the 'time' coordinate, and creates a 'year' coordinate. This could be achieved without using the available ``add_year`` by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AuxCoord(array([2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2007, 2007,\n",
      "       2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2008,\n",
      "       2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008,\n",
      "       2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,\n",
      "       2009, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010]), standard_name=None, units=Unit('1'), long_name='year')\n"
     ]
    }
   ],
   "source": [
    "def year_from_time(coord, point):\n",
    "    return coord.units.num2date(point).year\n",
    "\n",
    "coord_cat.add_categorised_coord(cube, 'year', cube.coord('time'),\n",
    "                                year_from_time)\n",
    "\n",
    "print(cube.coord('year'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cis_test]",
   "language": "python",
   "name": "conda-env-cis_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
